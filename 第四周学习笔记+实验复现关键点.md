

## 一、理论部分

### 1. 神经网络概述

#### 1.1 什么是神经网络
- 神经网络受人脑神经元结构启发，由多个连接的节点（神经元）组成
- 每个神经元接收输入，进行计算后产生输出
- 神经网络可以学习复杂的非线性关系，适合解决传统算法难以处理的问题

#### 1.2 神经网络的表示
- 神经网络由多个层组成：输入层、隐藏层和输出层
- 每层包含多个神经元
- 层与层之间通过权重和偏置项连接
- 使用上标 [l] 表示第 l 层的参数

### 2. 神经网络模型

#### 2.1 单个神经元计算
- 每个神经元计算：z = w^T x + b
- 使用激活函数获得输出：a = g(z)
- 常用激活函数：
  - Sigmoid函数：g(z) = 1/(1 + e^(-z))
  - ReLU函数：g(z) = max(0, z)
  - tanh函数：g(z) = (e^z - e^(-z))/(e^z + e^(-z))

#### 2.2 前向传播
- 输入层(l=0)：a[0] = X
- 隐藏层(l=1)：z[1] = W[1] * a[0] + b[1]，a[1] = g[1](z[1])
- 输出层(l=2)：z[2] = W[2] * a[1] + b[2]，a[2] = g[2](z[2])
- 其中 W[l] 是权重矩阵，b[l] 是偏置向量

#### 2.3 向量化实现
- 一次处理整个训练批次
- Z[l] = W[l] * A[l-1] + b[l]
- A[l] = g[l](Z[l])
- 其中 A[l] 的每一列对应一个样本

### 3. 激活函数

#### 3.1 sigmoid激活函数
- 公式：g(z) = 1/(1 + e^(-z))
- 范围：(0, 1)
- 适用于二元分类的输出层
- 缺点：在深度网络中容易导致梯度消失问题

#### 3.2 tanh激活函数
- 公式：g(z) = (e^z - e^(-z))/(e^z + e^(-z))
- 范围：(-1, 1)
- 通常优于sigmoid函数，在隐藏层中表现更好
- 仍然存在梯度消失问题

#### 3.3 ReLU激活函数
- 公式：g(z) = max(0, z)
- 范围：[0, +∞)
- 计算高效，解决了梯度消失问题
- 存在"死亡ReLU"问题（当输入为负时梯度为0）
- 目前最常用的隐藏层激活函数

#### 3.4 Leaky ReLU
- 公式：g(z) = max(0.01z, z)
- 解决了"死亡ReLU"问题
- 范围：(-∞, +∞)

#### 3.5 激活函数选择指南
- 输出层：
  - 二分类问题：sigmoid
  - 多分类问题：softmax
  - 回归问题：线性激活（无激活函数）
- 隐藏层：
  - 首选ReLU或Leaky ReLU
  - 其次考虑tanh

### 4. 多类分类与Softmax

#### 4.1 Softmax回归
- 用于多分类问题（C个类别）
- 计算过程：
  1. z[L] = W[L] * a[L-1] + b[L]
  2. t = e^(z[L])
  3. a[L] = t / sum(t)
- 输出向量中每个元素表示对应类别的概率，总和为1

#### 4.2 Softmax损失函数
- 交叉熵损失：L = -sum(y_i * log(a[L]_i))
- 其中y_i是真实标签的one-hot表示，a[L]_i是预测概率

### 5. 神经网络训练

#### 5.1 损失函数
- 二分类：J = -1/m * sum(y*log(a) + (1-y)*log(1-a))
- 多分类：J = -1/m * sum(sum(y_ij * log(a[L]_ij)))

#### 5.2 反向传播
- 计算损失函数对各参数的梯度
- 输出层误差：dZ[L] = A[L] - Y
- 反向传播计算每层误差：dZ[l] = (W[l+1])^T * dZ[l+1] * g'[l](Z[l])
- 参数梯度：
  - dW[l] = 1/m * dZ[l] * (A[l-1])^T
  - db[l] = 1/m * sum(dZ[l], axis=1)

#### 5.3 梯度下降更新
- W[l] = W[l] - α * dW[l]
- b[l] = b[l] - α * db[l]
- 其中α是学习率

## 二、实验部分

### 1. 二分类神经网络实验

#### 1.1 实验数据
- 数据集：花朵识别（例如"Planar Data"）
- 特征：2个（x1, x2），表示花的两个属性
- 标签：0或1，表示花的种类

#### 1.2 模型架构
- 2层神经网络（输入层-隐藏层-输出层）
- 隐藏层使用4个神经元，激活函数为tanh
- 输出层使用1个神经元，激活函数为sigmoid

#### 1.3 代码实现

```python
# 初始化参数
def initialize_parameters(n_x, n_h, n_y):
    W1 = np.random.randn(n_h, n_x) * 0.01
    b1 = np.zeros((n_h, 1))
    W2 = np.random.randn(n_y, n_h) * 0.01
    b2 = np.zeros((n_y, 1))
    
    parameters = {"W1": W1, "b1": b1, "W2": W2, "b2": b2}
    return parameters

# 前向传播
def forward_propagation(X, parameters):
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    
    Z1 = np.dot(W1, X) + b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = 1 / (1 + np.exp(-Z2))
    
    cache = {"Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2}
    return A2, cache

# 计算成本
def compute_cost(A2, Y):
    m = Y.shape[1]
    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y)
    cost = -1/m * np.sum(logprobs)
    return cost

# 反向传播
def backward_propagation(parameters, cache, X, Y):
    m = X.shape[1]
    
    W2 = parameters["W2"]
    
    A1 = cache["A1"]
    A2 = cache["A2"]
    
    dZ2 = A2 - Y
    dW2 = 1/m * np.dot(dZ2, A1.T)
    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)
    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))
    dW1 = 1/m * np.dot(dZ1, X.T)
    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)
    
    grads = {"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2}
    return grads

# 更新参数
def update_parameters(parameters, grads, learning_rate):
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    
    dW1 = grads["dW1"]
    db1 = grads["db1"]
    dW2 = grads["dW2"]
    db2 = grads["db2"]
    
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    
    parameters = {"W1": W1, "b1": b1, "W2": W2, "b2": b2}
    return parameters

# 神经网络模型
def nn_model(X, Y, n_h, num_iterations, learning_rate, print_cost=False):
    n_x = X.shape[0]
    n_y = Y.shape[0]
    
    parameters = initialize_parameters(n_x, n_h, n_y)
    
    for i in range(num_iterations):
        A2, cache = forward_propagation(X, parameters)
        cost = compute_cost(A2, Y)
        grads = backward_propagation(parameters, cache, X, Y)
        parameters = update_parameters(parameters, grads, learning_rate)
        
        if print_cost and i % 1000 == 0:
            print("Cost after iteration %i: %f" % (i, cost))
    
    return parameters

# 预测函数
def predict(parameters, X):
    A2, cache = forward_propagation(X, parameters)
    predictions = (A2 > 0.5)
    return predictions
```

#### 1.4 实验结果
- 隐藏层神经元数量对模型性能影响显著
- 神经元数量过少：欠拟合，无法捕捉复杂边界
- 神经元数量过多：可能过拟合，需要正则化
- 通过调整隐藏层大小，可以得到较好的决策边界

### 2. 深度神经网络实验

#### 2.1 模型架构
- L层神经网络（一个输入层，L-1个隐藏层，一个输出层）
- 激活函数：隐藏层使用ReLU，输出层使用sigmoid

#### 2.2 代码实现

```python
# 初始化参数
def initialize_parameters_deep(layer_dims):
    parameters = {}
    L = len(layer_dims)
    
    for l in range(1, L):
        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01
        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))
    
    return parameters

# 前向传播
def L_model_forward(X, parameters):
    caches = []
    A = X
    L = len(parameters) // 2
    
    for l in range(1, L):
        A_prev = A
        W = parameters['W' + str(l)]
        b = parameters['b' + str(l)]
        Z = np.dot(W, A_prev) + b
        A = np.maximum(0, Z)  # ReLU激活
        cache = (A_prev, W, b, Z)
        caches.append(cache)
    
    WL = parameters['W' + str(L)]
    bL = parameters['b' + str(L)]
    ZL = np.dot(WL, A) + bL
    AL = 1 / (1 + np.exp(-ZL))  # Sigmoid激活
    
    cache = (A, WL, bL, ZL)
    caches.append(cache)
    
    return AL, caches

# 计算成本
def compute_cost(AL, Y):
    m = Y.shape[1]
    cost = -1/m * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))
    return cost

# 反向传播
def L_model_backward(AL, Y, caches):
    grads = {}
    L = len(caches)
    m = AL.shape[1]
    Y = Y.reshape(AL.shape)
    
    # 输出层
    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))
    current_cache = caches[L-1]
    grads["dA" + str(L-1)], grads["dW" + str(L)], grads["db" + str(L)] = linear_activation_backward(
        dAL, current_cache, "sigmoid")
    
    # 隐藏层
    for l in reversed(range(L-1)):
        current_cache = caches[l]
        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(
            grads["dA" + str(l + 1)], current_cache, "relu")
        grads["dA" + str(l)] = dA_prev_temp
        grads["dW" + str(l + 1)] = dW_temp
        grads["db" + str(l + 1)] = db_temp
    
    return grads

# 更新参数
def update_parameters(parameters, grads, learning_rate):
    L = len(parameters) // 2
    
    for l in range(L):
        parameters["W" + str(l+1)] = parameters["W" + str(l+1)] - learning_rate * grads["dW" + str(l+1)]
        parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate * grads["db" + str(l+1)]
    
    return parameters
```

#### 2.3 实验结果
- 深度网络可以学习更复杂的特征表示
- 层数增加带来的挑战：梯度消失/梯度爆炸
- 引入批归一化和残差连接可以改善深度网络训练

## 三、关键概念总结

1. **神经网络基础**：神经网络由输入层、隐藏层和输出层组成，通过权重和偏置连接

2. **激活函数选择**：
   - 隐藏层：ReLU优于tanh优于sigmoid
   - 输出层：二分类用sigmoid，多分类用softmax，回归用线性

3. **前向传播**：从输入到输出逐层计算

4. **反向传播**：从输出到输入计算梯度并更新参数

5. **挑战与解决方案**：
   - 梯度消失/爆炸：合理初始化、使用ReLU激活函数
   - 过拟合：正则化、Dropout
   - 超参数调优：学习率、隐藏层大小、迭代次数

